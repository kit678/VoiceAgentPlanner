import asyncio
import sys
import os
from dotenv import load_dotenv
from loguru import logger

from pipecat.frames.frames import Frame, TextFrame
from pipecat.processors.frame_processor import FrameProcessor, FrameDirection
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.transports.local.audio import LocalAudioTransport, LocalAudioTransportParams
from pipecat.observers.loggers.transcription_log_observer import TranscriptionLogObserver

# Import Gemini Multimodal Live service
from pipecat.services.gemini_multimodal_live.gemini import (
    GeminiMultimodalLiveLLMService,
    InputParams,
    GeminiMultimodalModalities
)
from pipecat.transcriptions.language import Language

# Import WebSocket bridge
from websocket_server import bridge, start_websocket_bridge

# Import Frame Processors - using LLM-based intent parser
from llm.llm_intent_parser_processor import LLMIntentParserFrameProcessor
from llm.intent_parser_processor import IntentFrame  # Still need IntentFrame
from llm.command_processor_processor import CommandProcessorFrameProcessor
from firebase.firestore_service import FirestoreService

load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), '../../.env'), override=True)

logger.remove(0)
logger.add(sys.stderr, level="DEBUG")

# Custom processor to bridge Pipecat events to WebSocket
class WebSocketBridgeProcessor(FrameProcessor):
    def __init__(self):
        super().__init__()
        self.bridge = bridge
        
    async def process_frame(self, frame: Frame, direction: FrameDirection):
        # Call parent class method first
        await super().process_frame(frame, direction)
        
        # Forward relevant frames to WebSocket clients
        if isinstance(frame, TextFrame):
            if direction == FrameDirection.DOWNSTREAM:
                # TextFrames from STT will go to LLMIntentParserFrameProcessor
                pass
            elif direction == FrameDirection.UPSTREAM:
                # This is a response from LLM or CommandProcessorFrameProcessor
                await self.bridge.on_response_text(frame.text)
        elif isinstance(frame, IntentFrame) and direction == FrameDirection.DOWNSTREAM:
            # IntentFrames are handled by CommandProcessorFrameProcessor
            pass
        
        # Always pass the frame through the pipeline
        await self.push_frame(frame, direction)

    async def handle_text_from_ui(self, user_text: str):
        """Handles text commands sent from the UI via WebSocket, pushing them into the pipeline."""
        logger.debug(f"WebSocketBridgeProcessor received text from UI: {user_text}")
        # Push the user's text as a TextFrame into the pipeline for intent parsing
        await self.push_frame(TextFrame(user_text), FrameDirection.DOWNSTREAM)
        logger.debug(f"Pushed TextFrame: '{user_text}' into pipeline from UI.")

# Audio gate processor that can enable/disable audio flow
class AudioGateProcessor(FrameProcessor):
    def __init__(self):
        super().__init__()
        self._is_enabled = False
        
    async def process_frame(self, frame: Frame, direction: FrameDirection):
        # Call parent class method first (required by Pipecat)
        await super().process_frame(frame, direction)
        
        # Import frame types here to avoid circular imports
        from pipecat.frames.frames import AudioRawFrame, SystemFrame
        
        # Always pass non-audio frames through (like system control frames)
        if self._is_enabled or not isinstance(frame, AudioRawFrame):
            await self.push_frame(frame, direction)
        else:
            # Audio gate is closed - drop audio frames but log occasionally
            if hasattr(self, '_drop_count'):
                self._drop_count += 1
                if self._drop_count % 100 == 0:  # Log every 100 dropped frames
                    logger.debug(f"Audio gate: Dropped {self._drop_count} audio frames")
            else:
                self._drop_count = 1
                logger.debug("Audio gate: Started dropping audio frames")
        
    async def enable(self):
        """Enable audio flow"""
        if not self._is_enabled:
            logger.info("Audio gate: ENABLED - Audio will flow through")
            self._is_enabled = True
            if hasattr(self, '_drop_count'):
                logger.info(f"Audio gate: Stopped dropping audio frames (dropped {self._drop_count} total)")
                delattr(self, '_drop_count')
            
    async def disable(self):
        """Disable audio flow"""
        if self._is_enabled:
            logger.info("Audio gate: DISABLED - Audio will be blocked")
            self._is_enabled = False
            
    @property
    def is_enabled(self):
        return self._is_enabled

async def main():
    logger.info("Starting Pipecat pipeline with LLM-based intent recognition...")

    # Start WebSocket server
    websocket_server = await start_websocket_bridge()
    logger.info("WebSocket bridge started on localhost:8765")

    # Configure Audio Transport
    transport_params = LocalAudioTransportParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
    )
    transport = LocalAudioTransport(transport_params)

    # Create audio gate (starts disabled)
    audio_gate = AudioGateProcessor()

    # SINGLE SERVICE: Gemini handles BOTH LLM reasoning AND TTS
    gemini_service = GeminiMultimodalLiveLLMService(
        api_key=os.getenv("GOOGLE_API_KEY"),
        model="models/gemini-2.0-flash-live-001",
        voice_id="Puck",
        transcribe_user_audio=True,
        system_instruction="You are a helpful voice assistant. Keep responses brief and natural for speech. Avoid special characters that might cause audio issues.",
        params=InputParams(
            temperature=0.7,
            language=Language.EN_US,
            modalities=GeminiMultimodalModalities.AUDIO
        )
    )

    # Create Frame Processors - using LLM-based intent parser
    llm_intent_parser_fp = LLMIntentParserFrameProcessor(gemini_service)
    firestore_service = FirestoreService()
    command_processor_fp = CommandProcessorFrameProcessor(firestore_service)

    # Create WebSocket bridge processor
    websocket_processor = WebSocketBridgeProcessor()

    # LLM-BASED PIPELINE:
    # Audio Input -> Audio Gate -> Gemini (STT) -> LLM Intent Parser FP -> Command Processor FP -> Gemini (TTS) -> WebSocket Bridge -> Audio Output
    # Text Input (from UI) -> WebSocket Bridge -> LLM Intent Parser FP -> Command Processor FP -> WebSocket Bridge (for text response)

    # Key difference: Instead of rule-based IntentParserFrameProcessor, we use LLMIntentParserFrameProcessor
    # which leverages the same Gemini service for intent classification and parameter extraction

    pipeline = Pipeline([
        transport.input(),          # Audio input from microphone
        audio_gate,                 # Audio gate (controllable)
        gemini_service,             # Gemini: STT (produces TextFrame), LLM, TTS (consumes TextFrame, produces AudioFrame)
        # After Gemini STT, TextFrame goes downstream to LLM intent parser
        llm_intent_parser_fp,       # LLM-based: Consumes TextFrame, uses Gemini for intent classification, produces IntentFrame
        command_processor_fp,       # Consumes IntentFrame, produces TextFrame (upstream for TTS/response)
        # The TextFrame from CommandProcessorFP goes to Gemini for TTS (upstream) or WebSocketBridge for UI
        websocket_processor,        # Bridges frames to/from WebSocket for Electron UI
        transport.output()          # Audio output to speakers
    ])

    # Attach audio gate to pipeline for WebSocket control
    pipeline.audio_gate = audio_gate
    
    # Connect pipeline and text input handler to bridge
    bridge.set_pipeline(pipeline)
    bridge.set_text_input_handler(websocket_processor.handle_text_from_ui)

    runner_params = {}
    if sys.platform == "win32":
        logger.info("Disabling PipelineRunner SIGINT handling on Windows.")
        runner_params["handle_sigint"] = False

    runner = PipelineRunner(**runner_params)
    logger.info("Starting LLM-based pipeline: Audio -> Gemini Live -> LLM Intent Parser -> WebSocket Bridge -> Audio")
    
    task = PipelineTask(
        pipeline,
        params=PipelineParams(allow_interruptions=True, enable_metrics=True),
        observers=[TranscriptionLogObserver()]
    )
    
    try:
        await runner.run(task)
    finally:
        # Clean up WebSocket server
        websocket_server.close()
        await websocket_server.wait_closed()
        logger.info("WebSocket server closed")

if __name__ == "__main__":
    if sys.platform == "win32":
        try:
            asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
            logger.debug("WindowsSelectorEventLoopPolicy set.")
        except Exception as e:
            logger.warning(f"Could not set WindowsSelectorEventLoopPolicy: {e}")
    
    try:
        logger.debug(f"Current asyncio event loop policy: {type(asyncio.get_event_loop_policy()).__name__}")
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("Application terminated by user (KeyboardInterrupt).")
    except Exception as e:
        logger.error(f"An error occurred: {type(e).__name__} - {repr(e)}")
        import traceback
        logger.error(traceback.format_exc())